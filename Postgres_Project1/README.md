# Summary of project

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app

In order to enable Sparkify to analyze their data, a Relational database needs to be created with appropriate ETL pipeline

# How to run the python scripts

### Prerequisites

Python3 is recommended as the environment. the commands need to be run fromt command line.


To create the database tables and run the ETL pipeline, you must run the following two files in the order that they are listed below

To create tables:
```bash
python3 create_tables.py
```
To populate tables via ETL:
```bash
python3 etl.py
```


# Data Files in the repository

* **[data](./data)**: Folder containing data of songs and logs 

This Project-1 handles data of a music streaming startup, Sparkify. Data set is a set of files in JSON format and contains two parts:

* **./data/song_data**: static data about artists and songs
* **./data/log_data**: event data of service usage e.g. who listened what song, when, where, and with which client



Project builds an ETL pipeline (Extract, Transform, Load) to create the DB and tables, fetch data from JSON files, process the data, and insert the the data to DB. As technologies, Project-1 uses python, SQL, Postgresql DB.

# ETL code in the repository

* **[create_tables.py](./create_tables.py)**: Python script to perform SQL-Statements for (re-)creating database and tables
* **[sql_queries.py](./sql_queries.py)**: Python script containing SQL-Statements used by create_tables.py and etl.py
* **[etl.py](./etl.py)**: Python script to extract the needed information from Song and Log data inside the data folder and parsing/inserting them to the created database schema and tables

Data Set  (results after running the etl.py):

* ./data/song_data: 71 files
* ./data/log_data: 31 files
* songplays: 6831
* (unique) user: 97
* songs: 71
* artists: 69


# The purpose of this database

Sparkify analytics database (called here sparkifydb) schema has a star design. Start design means that it has one Dimension Table having business data, and supporting Fact Tables. Dimension Table answers one of the key questions: what songs users are listening to.

### Fact Table

* **songplays**: song play data together with user, artist, and song info (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

### Dimension Tables

* **users**: user info (columns: user_id, first_name, last_name, gender, level)
* **songs**: song info (columns: song_id, title, artist_id, year, duration)
* **artists**: artist info (columns: artist_id, name, location, latitude, longitude)
* **time**: detailed time info about song plays (columns: start_time, hour, day, week, month, year, weekday)


# Database schema design and ETL pipeline.

In order to enable Sparkify to analyze their data, a Relational Database Schema was created, and a corresponding ETL diagram to source data from the files and populate the tables.

The so-called star scheme enables the company to view the user behaviour over several dimensions.
The fact table is used to store all user song activities that contain the category "NextSong". Using this table, the company can relate and analyze the dimensions users, songs, artists and time.

In order to popoulate the relational database, an ETL pipeline is used, which makes it possible to extract the necessary information from the log files of the user behaviour as well as the corresponding master data of the songs and convert it into the schema that we have created for Spotify.


# Dataset used

Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.


# End result of project
Provide customer startup Sparkify tools to analyse their service data and help them answer their key business questions like _"What songs users are listening to"_.